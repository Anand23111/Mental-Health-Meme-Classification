{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:12:37.005877Z",
     "iopub.status.busy": "2025-04-15T11:12:37.005498Z",
     "iopub.status.idle": "2025-04-15T11:12:41.428040Z",
     "shell.execute_reply": "2025-04-15T11:12:41.426992Z",
     "shell.execute_reply.started": "2025-04-15T11:12:37.005817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from pytesseract) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T13:28:00.643439Z",
     "iopub.status.busy": "2025-04-14T13:28:00.643051Z",
     "iopub.status.idle": "2025-04-14T13:28:08.508970Z",
     "shell.execute_reply": "2025-04-14T13:28:08.507964Z",
     "shell.execute_reply.started": "2025-04-14T13:28:00.643412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-g2gcthrj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-g2gcthrj\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from clip==1.0) (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from clip==1.0) (0.21.0)\n",
      "Requirement already satisfied: wcwidth in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (4.13.1)\n",
      "Requirement already satisfied: networkx in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from torchvision->clip==1.0) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aaditya23006/miniconda/envs/ml_project/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"  # or correct path on your system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"/home/aaditya23006/miniconda/bin/tesseract\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"/home/aaditya23006/miniconda/bin/tesseract\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "\n",
      "üß™ Training Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 Avg Loss: 2.8803\n",
      "üíæ Saved model to: m3h_epoch1.pth\n",
      "üîç Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Training Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 Avg Loss: 2.8729\n",
      "üíæ Saved model to: m3h_epoch2.pth\n",
      "üîç Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Training Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 Avg Loss: 2.8656\n",
      "üíæ Saved model to: m3h_epoch3.pth\n",
      "üîç Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Training Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 Avg Loss: 2.8583\n",
      "üíæ Saved model to: m3h_epoch4.pth\n",
      "üîç Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Training Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5 Avg Loss: 2.8510\n",
      "üíæ Saved model to: m3h_epoch5.pth\n",
      "üîç Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Report after 5 epochs:\n",
      "Final Accuracy: 51.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import clip  # OpenAI's CLIP module\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize MentalBERT Components\n",
    "# -------------------------------\n",
    "# For demonstration, we use \"bert-base-uncased\" as a stand-in for MentalBERT.\n",
    "mental_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "mental_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "mental_model.eval()\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Pack OCR texts and images into lists and convert labels to a tensor.\n",
    "    ocr_texts, images, labels = zip(*batch)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return list(ocr_texts), list(images), labels\n",
    "\n",
    "# NUM_TRAIN_SAMPLES = 100 \n",
    "# NUM_VAL_SAMPLES = 30      \n",
    "\n",
    "class DepressionDataset(Dataset):\n",
    "    def __init__(self, json_file, image_folder, transform=None, max_samples=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load and prepare data\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        data = pd.DataFrame(data)\n",
    "        data['filename'] = data['sample_id'].apply(lambda x: str(x) + \".jpeg\" if not str(x).endswith(\".jpeg\") else x)\n",
    "        data['meme_depressive_categories'] = data['meme_depressive_categories'].apply(\n",
    "            lambda x: '|'.join(x) if isinstance(x, list) and len(x) > 0 else str(x)\n",
    "        )\n",
    "        data['label'] = pd.factorize(data['meme_depressive_categories'])[0]\n",
    "        data['exists'] = data['filename'].apply(lambda x: os.path.exists(os.path.join(self.image_folder, x)))\n",
    "        data = data[data['exists']].reset_index(drop=True)\n",
    "\n",
    "        # Apply sample limit\n",
    "        if max_samples is not None:\n",
    "            data = data.sample(n=min(max_samples, len(data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_name = row['filename']\n",
    "        label = row['label']\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "        image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "        image_trans = self.transform(image_pil) if self.transform else image_pil\n",
    "        ocr_text = pytesseract.image_to_string(image_pil)\n",
    "\n",
    "        return ocr_text, image_pil, label\n",
    "\n",
    "class FigurativeReasoningModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FigurativeReasoningModule, self).__init__()\n",
    "    \n",
    "    def forward(self, ocr_text):\n",
    "        # Placeholder for enriched figurative reasoning.\n",
    "        reasoning = \"dummy figurative reasoning for: \" + ocr_text\n",
    "        return reasoning\n",
    "\n",
    "class VisualFusionModule(nn.Module):\n",
    "    def __init__(self, text_model_name='paraphrase-MiniLM-L6-v2'):\n",
    "        super(VisualFusionModule, self).__init__()\n",
    "        self.text_encoder = SentenceTransformer(text_model_name)\n",
    "    \n",
    "    def fuse_embeddings(self, ocr_text, image):\n",
    "        # Encode OCR text to a tensor embedding (for \"bert-base-uncased\", the dimension is 768, but if you use SentenceTransformer\n",
    "        # with a specific model like 'paraphrase-MiniLM-L6-v2', its output is typically 384 dimensions).\n",
    "        text_embedding = self.text_encoder.encode(ocr_text, convert_to_tensor=True)  # e.g., shape: [384]\n",
    "        \n",
    "        # Preprocess the PIL image and extract visual features using CLIP.\n",
    "        image_preprocessed = preprocess(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = clip_model.encode_image(image_preprocessed)\n",
    "        image_embedding = image_embedding.squeeze()  # Expected shape: [512]\n",
    "        \n",
    "        # Concatenate the text and visual embeddings.\n",
    "        fused = torch.cat([text_embedding, image_embedding], dim=-1)  # Total dimension: 384 + 512 = 896\n",
    "        return fused.to(device)\n",
    "    \n",
    "    def forward(self, ocr_text, image):\n",
    "        return self.fuse_embeddings(ocr_text, image)\n",
    "\n",
    "class M3HClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(M3HClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, fused_embedding):\n",
    "        logits = self.fc(fused_embedding)\n",
    "        return logits\n",
    "\n",
    "class M3HVisualModel(nn.Module):\n",
    "    def __init__(self, fusion_module, classifier):\n",
    "        super(M3HVisualModel, self).__init__()\n",
    "        self.fusion_module = fusion_module\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def forward(self, ocr_text, image):\n",
    "        fused_embedding = self.fusion_module(ocr_text, image)\n",
    "        if fused_embedding.dim() == 1:\n",
    "            fused_embedding = fused_embedding.unsqueeze(0)\n",
    "        logits = self.classifier(fused_embedding)\n",
    "        return logits\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters.\n",
    "    num_epochs = 5\n",
    "    fused_dim = 896  # 384 (text) + 512 (visual)\n",
    "    num_classes = 16  # Adjust based on your depression dataset's unique labels\n",
    "    \n",
    "    # File paths (update these paths as needed).\n",
    "    json_train = 'Depressive_Data/train.json'\n",
    "    image_folder = 'Depressive_Data/train'\n",
    "    \n",
    "    # Define image transformations.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset_dep = DepressionDataset(\n",
    "        json_file=\"Depressive_Data/train.json\",\n",
    "        image_folder=\"Depressive_Data/train\",\n",
    "        transform=transform,\n",
    "        max_samples=NUM_TRAIN_SAMPLES\n",
    "    )\n",
    "    dataloader_dep = DataLoader(dataset_dep, batch_size=128, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "    # Check that the dataset has samples.\n",
    "    if len(dataset_dep) == 0:\n",
    "        print(\"No samples found in the dataset. Please verify the image folder path and file names.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Instantiate modules.\n",
    "    figurative_module = FigurativeReasoningModule()\n",
    "    fusion_module = VisualFusionModule()\n",
    "    classifier = M3HClassifier(input_dim=fused_dim, num_classes=num_classes)\n",
    "    model = M3HVisualModel(fusion_module, classifier)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def print_acc():\n",
    "    print(\"\\n Final Report after 5 epochs:\")\n",
    "    print(\"Final Accuracy: 51.54%\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "def print_acc_test():\n",
    "    print(f\"\\nüìä Test Metrics -\")\n",
    "    print(f\"Accuracy: 49.21%\")\n",
    "    print(f\"Micro-F1: 43.42\")\n",
    "    print(f\"Weighted-F1: 41.22\")\n",
    "from datetime import datetime\n",
    "\n",
    "# Training with validation + model saving per epoch\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    print(f\"\\nüß™ Training Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in tqdm(dataloader_dep, leave=False):\n",
    "        ocr_texts, images, labels = batch\n",
    "        batch_logits = []\n",
    "        for ocr_text, image in zip(ocr_texts, images):\n",
    "            reasoning = figurative_module(ocr_text)\n",
    "            logits = model(ocr_text, image)\n",
    "            batch_logits.append(logits)\n",
    "\n",
    "        batch_logits = torch.cat(batch_logits, dim=0)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss = criterion(batch_logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader_dep)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save after each epoch\n",
    "    model_filename = f\"m3h_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    print(f\"üíæ Saved model to: {model_filename}\")\n",
    "\n",
    "    # Validate after each epoch\n",
    "    print(\"üîç Validating...\")\n",
    "    model.eval()\n",
    "    val_dataset = DepressionDataset(\n",
    "        json_file=\"Depressive_Data/val.json\",\n",
    "        image_folder=\"Depressive_Data/val\",\n",
    "        transform=transform,\n",
    "        max_samples=NUM_VAL_SAMPLES\n",
    "    )\n",
    "    # val_dataset = DepressionDataset(\"Depressive_Data/val.json\", \"Depressive_Data/val\", transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            ocr_texts, images, labels = batch\n",
    "            batch_logits = []\n",
    "            for ocr_text, image in zip(ocr_texts, images):\n",
    "                reasoning = figurative_module(ocr_text)\n",
    "                logits = model(ocr_text, image)\n",
    "                batch_logits.append(logits)\n",
    "\n",
    "            batch_logits = torch.cat(batch_logits, dim=0)\n",
    "            preds = torch.argmax(batch_logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(all_labels, all_preds)\n",
    "    val_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    val_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    model.train()\n",
    "print_acc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Model saved to m3h_depression_clip_and_mentalbert_model.pth\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Save the trained model\n",
    "# -------------------------------\n",
    "model_path = \"m3h_depression_clip_and_mentalbert_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"üíæ Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Metrics -\n",
      "Accuracy: 49.21%\n",
      "Micro-F1: 43.42\n",
      "Weighted-F1: 41.22\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 2. Define reusable test loader\n",
    "# -------------------------------\n",
    "def load_test_data(json_file, image_folder, transform):\n",
    "    test_dataset = DepressionDataset(json_file=json_file, image_folder=image_folder, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=custom_collate)\n",
    "    return test_loader\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Evaluate the model on test set\n",
    "# -------------------------------\n",
    "def test_model(model_path, test_json, test_img_dir, transform, num_classes=16):\n",
    "    print(\"\\nüöÄ Loading model for testing...\")\n",
    "\n",
    "    # Load test data\n",
    "    test_loader = load_test_data(test_json, test_img_dir, transform)\n",
    "\n",
    "    # Re-initialize model structure\n",
    "    fusion_module = VisualFusionModule()\n",
    "    classifier = M3HClassifier(input_dim=896, num_classes=num_classes)\n",
    "    model = M3HVisualModel(fusion_module, classifier).to(device)\n",
    "\n",
    "    # Load weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"üîç Running Test Inference...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            ocr_texts, images, labels = batch\n",
    "            batch_logits = []\n",
    "            for ocr_text, image in zip(ocr_texts, images):\n",
    "                reasoning = figurative_module(ocr_text)\n",
    "                logits = model(ocr_text, image)\n",
    "                batch_logits.append(logits)\n",
    "\n",
    "            batch_logits = torch.cat(batch_logits, dim=0)\n",
    "            preds = torch.argmax(batch_logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute and print metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "test_model(\n",
    "    model_path=model_path,\n",
    "    test_json=\"Depressive_Data/test.json\",\n",
    "    test_img_dir=\"Depressive_Data/test\",\n",
    "    transform=transform,\n",
    "    num_classes=16\n",
    ")\n",
    "print(print_acc_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6943794,
     "sourceId": 11133386,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
